{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "838h8QmzyarI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model 2\n",
        "#Importing dependencies\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import time\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import datetime\n",
        "import pickle\n",
        " \n",
        "#Sets batch size (number of images inputted at one time) for training\n",
        "batch_size = 1000\n",
        " \n",
        "#Naming the model and adding a TensorBoard callback for graphs of metrics\n",
        "NAME = \"Malaria-Images-CNN-V1\".format(int(time.time()))\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
        " \n",
        "#Loading in the data from Google Drive\n",
        "pickle_in = open(\"/content/drive/My Drive/Science Fair/Malaria Project/X.pickle\",\"rb\")\n",
        "X = pickle.load(pickle_in)\n",
        "pickle_in = open(\"/content/drive/My Drive/Science Fair/Malaria Project/y.pickle\",\"rb\")\n",
        "y = pickle.load(pickle_in)\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "X = X/255.0\n",
        " \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42) #Splits data into train and test\n",
        " \n",
        "#Defining the model\n",
        "model = Sequential()\n",
        " \n",
        "#Adding the layers in the model\n",
        "#First convolution and pooling block\n",
        "model.add(Conv2D(128, (3, 3), input_shape=X.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        " \n",
        "#Second convolution and pooling block\n",
        "model.add(Conv2D(128, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        " \n",
        "#Flattens the stacks of features down\n",
        "model.add(Flatten())\n",
        " \n",
        "#Fully connected (regular) layer\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        " \n",
        "#Output layer\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        " \n",
        "#Compiling model & adding metrics (accuracy, precision, recall)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        " \n",
        "#Training the model\n",
        "model.fit(X_train, y_train,batch_size=batch_size, epochs=20, validation_split=0.3, callbacks=[tensorboard])\n",
        " \n",
        "#Evaluating the model on unseen data\n",
        "scores = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(scores)\n",
        "print(model.metrics_names)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}